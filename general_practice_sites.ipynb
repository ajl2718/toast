{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623813a0",
   "metadata": {},
   "source": [
    "# Australian General Practice location scraper\n",
    "\n",
    "5 October 2021\n",
    "\n",
    "---\n",
    "\n",
    "**Description**\n",
    "\n",
    "It is difficult to find existing datasets for general practice locations in Australia (e.g., from data.gov.au or data.vic.gov.au). Healthdirect appears to be the best official source of health provider locations. \n",
    "\n",
    "This scraper contains some basic code to scrape healthdirect.gov.au to get the names and locations of all General Practice sites in Victoria. It can be modified to obtain all those in any other given state, the whole country or just a subset of postcodes.\n",
    "\n",
    "\n",
    "**Instructions**\n",
    "- Make sure that pandas, requests and lxml are installed\n",
    "- Ensure that the reference postcode data is specified correctly\n",
    "- Run each of the cells in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0e6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from time import sleep\n",
    "\n",
    "import json\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d25b84",
   "metadata": {},
   "source": [
    "## 1. Define paths and utility functions\n",
    "\n",
    "- Specify the required data elements from the website\n",
    "- Create utility function to extract data elements from web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cb6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these definesthe elements within the webpage where name and location information is recorded\n",
    "path_name = './/div[@class=\"veyron-hsf-page \"]/a/@href'\n",
    "path_address1 = './/div[@class=\"veyron-hsf-page \"]/a/@data-address'\n",
    "path_lat = './/div[@class=\"veyron-hsf-page \"]/a/@data-lat'\n",
    "path_long = './/div[@class=\"veyron-hsf-page \"]/a/@data-long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b80742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pages of the individual practices, these are the paths with relevant information\n",
    "path_address2 = './/p[@class=\"hsf-service_details-data-address veyron-hsf-full-address\"]/@data-full-address'\n",
    "path_opening_hours_day = './/div[@class=\"hsf-service_details-data-hours-group\"]/ul/li/span[@class=\"hsf-service_details-data-hours-weekday\"]/text()'\n",
    "path_opening_hours_time = './/div[@class=\"hsf-service_details-data-hours-group\"]/ul/li/span[@class=\"hsf-service_details-data-hours-times\"]/text()'\n",
    "path_fees = './/p[@class=\"hsf-service_details-data-billing\"]/text()'\n",
    "path_details = './/p[@class=\"hsf-service_details-data-description\"]/span/text()'\n",
    "\n",
    "path_javascript = './/script[@type=\"text/javascript\"]/text()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43ad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data elements that we are interested in\n",
    "data_scheme = {'Name': path_name,\n",
    "               'Address': path_address1,\n",
    "               'Latitude': path_lat,\n",
    "               'Longitude': path_long}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad243433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_js(js_text, start_marker='[{\"', end_marker='}],\\n', offset=2):\n",
    "    \"\"\"\n",
    "    Extract the javascript text and convert to dict. This contains lat, long etc\n",
    "    \"\"\"\n",
    "    start_index, end_index = js_text.find(start_marker), js_text.find(end_marker)\n",
    "    output = js_text[start_index:end_index+offset]\n",
    "    return json.loads(output)\n",
    "\n",
    "def get_page_info(url):\n",
    "    \"\"\"\n",
    "    Extract all the info we are interested in for a given URL\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.text)\n",
    "    address = tree.xpath(path_address1)\n",
    "    opening_hours_day = tree.xpath(path_opening_hours_day)\n",
    "    opening_hours_time = tree.xpath(path_opening_hours_time)\n",
    "    fees = tree.xpath(path_fees)\n",
    "    js_text = tree.xpath(path_javascript)\n",
    "    js_data = extract_js(js_text[9])\n",
    "    \n",
    "    details = ' '.join(tree.xpath(path_details)).strip()\n",
    "    \n",
    "    latitude = float(js_data[0]['location']['physicalLocation']['geocode']['latitude'])\n",
    "    longitude = float(js_data[0]['location']['physicalLocation']['geocode']['longitude'])\n",
    "    \n",
    "    # create output dataframe\n",
    "    df_temp = pd.DataFrame(data={'url': url, 'address': address, 'fees': ' '.join(fees).strip(), \n",
    "                                 'details': details,\n",
    "                                 'latitude': latitude, 'longitude': longitude})\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "def get_page_info(url):\n",
    "    \"\"\"\n",
    "    Extract all the info we are interested in for a given URL\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.text)\n",
    "    good_text = tree.xpath('.//script[@type=\"text/javascript\"]/text()')[9]\n",
    "    good_data = find_json(good_text)\n",
    "    \n",
    "    location = pd.json_normalize(good_data['location'])\n",
    "    billing = pd.json_normalize(good_data['billingOptions'])\n",
    "    name = good_data['organisation']['name']\n",
    "    opening_hours = pd.json_normalize(good_data['calendar']['openRule']).stack().reset_index()\n",
    "    opening_hours[0] = opening_hours[0].astype(str)\n",
    "    opening_hours.index = opening_hours['level_1'] + opening_hours['level_0'].astype(str)\n",
    "    opening_hours = opening_hours[[0]].T.reset_index().iloc[:, 1:]\n",
    "    \n",
    "    location_cleaned = (\n",
    "        location[['physicalLocation.addressLine3', \n",
    "                  'physicalLocation.postcode',\n",
    "                  'physicalLocation.suburb.label', \n",
    "                  'physicalLocation.state.label', \n",
    "                  'physicalLocation.geocode.latitude', \n",
    "                  'physicalLocation.geocode.longitude']]\n",
    "        .rename(columns={'physicalLocation.addressLine3': 'street_address', \n",
    "                         'physicalLocation.postcode': 'postcode', \n",
    "                         'physicalLocation.suburb.label': 'suburb', \n",
    "                         'physicalLocation.state.label': 'state', \n",
    "                         'physicallocation.geocode.latitude': 'latitude', \n",
    "                         'physicalLocation.geocode.longitude': 'longitude'})\n",
    "    )\n",
    "    \n",
    "    billing_cleaned = billing[['valueType.label']].rename(columns={'valueType.label': 'fees'})\n",
    "    df_temp = pd.concat([location_cleaned, billing_cleaned, opening_hours], axis=1)\n",
    "    df_temp['name'] = name\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bca3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations(text, data_scheme):\n",
    "    \"\"\"\n",
    "    Retrieve location information in web page\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    text (str): the scraped data from the webpage containing\n",
    "    required information\n",
    "    data_scheme (dict): specifies paths for each data element\n",
    "    \n",
    "    Returns\n",
    "    pd.DataFrame with the parsed data\n",
    "    \"\"\"\n",
    "    tree = html.fromstring(text)\n",
    "    names = data_scheme.keys()\n",
    "    output_data = dict()\n",
    "    \n",
    "    for name in names:\n",
    "        path = data_scheme[name]\n",
    "        item_data = tree.xpath(path)\n",
    "        output_data[name] = item_data\n",
    "        \n",
    "    return pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c383ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json(text):\n",
    "    \"\"\"\n",
    "    Given some text that contains JSON, find where it beings and ends\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    text (str): the input text\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    the json text\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize starting and ending indices\n",
    "    start = 0\n",
    "    end = -1\n",
    "    \n",
    "    # initialize the depth inside the JSON nested structure\n",
    "    current_level = -1\n",
    "    \n",
    "    # for each character in the text\n",
    "    # look for '{' or '}'\n",
    "    # raise or lower the current_level \n",
    "    for n in range(0, len(text)):\n",
    "        current_char = text[n]\n",
    "        \n",
    "        if current_char == '{':\n",
    "            # if we have not yet seen a '{' then initialize the start index\n",
    "            # along with the current_level\n",
    "            if current_level == -1:\n",
    "                start = n\n",
    "                current_level = 1\n",
    "            else:\n",
    "                current_level += 1\n",
    "        elif current_char == '}':\n",
    "            current_level -= 1\n",
    "            # if we first encounter level 0 then this\n",
    "            # must be the end of the JSON\n",
    "            if current_level == 0:\n",
    "                end = n+1\n",
    "                break\n",
    "                \n",
    "    return json.loads(text[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f45f2",
   "metadata": {},
   "source": [
    "## 2. Create list of suburb-postcode pairs\n",
    "\n",
    "Using reference postcode data, create the suburb-postcode pairs that we require to scrape location data for. This is specified in the ```folder``` and ```filename``` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55861d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/alex/Desktop/Data/reference_data'\n",
    "filename = f'{folder}/postcodes_scraped_221021.csv'\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8143e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are only interested in the VIC postcodes (assuming that that file has a 'state' column)\n",
    "df_vic = df.query('state == \"VIC\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f0578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_postcodes = (\n",
    "    df\n",
    "    .query('state==\"VIC\"') # only want vic suburbs\n",
    "    .assign(suburb_formatted=df.suburb.str.replace(' ', '_'))\n",
    "    .assign(suburb_postcode=lambda df_: df_.suburb_formatted.str.lower() + '-' + df_.postcode.astype(str))\n",
    "    .suburb_postcode.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120cd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(range(1,20)) # how many pages do we scrape for each of the suburb-postcode pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21e074",
   "metadata": {},
   "source": [
    "## 3. Create URL list to scrape\n",
    "\n",
    "Create the list of URLs to scrape based on the suburb-postcode pairs that are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ab8e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic url\n",
    "url_base = 'https://www.healthdirect.gov.au/australian-health-services/results/'\n",
    "url_base2 = 'https://www.healthdirect.gov.au'\n",
    "url_middle = '/tihcs-aht-11222/gp-general-practice?pageIndex='\n",
    "url_end = '&tab=SITE_VISIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efddfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of URLs to scrape\n",
    "urls = [f'{url_base}{suburb_postcode}{url_middle}{n}{url_end}' \n",
    "        for suburb_postcode in suburb_postcodes \n",
    "        for n in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6560c66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3183, 12732)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suburb_postcodes), len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92ce35",
   "metadata": {},
   "source": [
    "## 4. Scrape the data to initial set of data for all the locations\n",
    "\n",
    "For each of the URLs in the list ```urls``` request the data, parse it and append to list of dataframes ```df_list```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231c1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12732 URLs.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(urls)} URLs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a15f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "urls_bad = []\n",
    "\n",
    "# in case the request returns an error, try again\n",
    "# this makes the scraper more robust\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    backoff=0.5\n",
    ")\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# loop through URLs scrape each one and parse data\n",
    "for n, url in enumerate(urls[3575:6000]):\n",
    "    if n % 100 == 0:\n",
    "        sleep(5)\n",
    "    print(f'{n}: Scraping URL: {url}')\n",
    "    page = http.get(url)\n",
    "    print(f'status code: {page.status_code}')\n",
    "    if page.status_code == 200:\n",
    "        output_data = get_locations(page.text, data_scheme)\n",
    "    else:\n",
    "        print('Unsuccessful request')\n",
    "        urls_bad.append(url)\n",
    "    df_list.append(pd.DataFrame(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb2c5d",
   "metadata": {},
   "source": [
    "Concatenate all the datasets for each suburb-postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce26be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = pd.concat(df_list).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10620ab1",
   "metadata": {},
   "source": [
    "## 5. Tidy up the data\n",
    "\n",
    "- Remove unneccessary white spaces\n",
    "- drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the scraped data\n",
    "df_cleaned = (\n",
    "    df_scraped\n",
    "    .assign(Name=df_scraped.Name.str.strip()) # remove unncessary characters from name\n",
    "    .assign(Address=df_scraped.Address.str.strip()) # remove unncessary characters from Address\n",
    "    .drop_duplicates()\n",
    "    .assign(suburb=df_scraped.Address.str.extract(r'(\\, )([A-Z]{2,}\\s{0,1}[A-Z]+)')[1]) # extract the suburb\n",
    "    .assign(postcode=df_scraped.Address.str.extract(r'([0-9]{4})\\s{0,1}$')) # extract the postcode\n",
    "    .reset_index()\n",
    "    .iloc[:, 1:]\n",
    ")\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab08a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/home/alex/Desktop/Data/scraped/gp_locations'\n",
    "\n",
    "df_final = df_cleaned.drop_duplicates().reset_index().iloc[:, 1:]\n",
    "df_final.to_csv(f'{output_folder}/gp_scraped_071022_partial1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c63f24",
   "metadata": {},
   "source": [
    "## 6. Scrape additional info from each of the practices (optional)\n",
    "\n",
    "This scrapes the individual URLs of each of the practices to get the following additional information:\n",
    "\n",
    "- Fees\n",
    "- Opening hours\n",
    "- Other details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9aad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the practices, scrape the additional detailed info\n",
    "df_all_details_temp = []\n",
    "\n",
    "for url in df_cleaned['Name'].values:\n",
    "    print(f'\\nRetrieving detailed info for {url}')\n",
    "    url_name = f\"{url_base2}{url}\"\n",
    "    df_temp = get_page_info(url_name)\n",
    "    df_all_details_temp.append(df_temp)\n",
    "    \n",
    "df_all_details = pd.concat(df_all_details_temp).drop_duplicates().reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cebeb",
   "metadata": {},
   "source": [
    "## 7. Output this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_details.to_csv(f'{output_folder}/gp_locations_detailed_info_081022.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
